---
layout: post
date: 2016-01-07
description: K-Means from scratch using Python
categories:
- code
- data-mining
- clustering
permalink: blog/kmeans-part-1
headline: blog
heading: K-Means | Part I
---
In this article we will build a [k-means clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering). We will use plain old Python and no other third party code. The goal of this article is to convey the basic structure of the k-means algorithm by developing a minimal working example capable of clustering a 2D dataset. It is expected that you understand basic programming, simple vector math and that you have at least some idea about how k-means works. Let's get started.

I did a quick Google search for `2d dataset` and found some [useful datasets](https://people.sc.fsu.edu/~jburkardt/datasets/spaeth/spaeth.html) made specifically to test clustering algorithms.

Below is the data from the first file:

```(41, 45),(39, 44),(42, 43),(44, 43),(10, 42),(38, 42),(8, 41),(41, 41),(13, 40),(45, 40),(7, 39),(38, 39),(42, 39),(9, 38),(12, 38),(19, 38),(25, 38),(6, 37),(13, 35),(9, 34),(12, 34),(32, 27),(26, 25),(39, 24),(34, 23),(37, 23),(22, 22),(38, 21),(35, 20),(31, 18),(26, 16),(38, 13),(29, 11),(34, 11),(37, 10),(40, 9),(42, 9)
```

As you can see these data consist of simple (x, y) pairs. We need a way to represent these pairs in Python. We define a two dimensional vector class to store each of the points. This class will also overload common arithmetic operations such as addition, subtraction, multiplication and division, as well as other common vector functions that will be useful in our algorithm.

<script src="https://gist.github.com/jeremynealbrown/4de42f45d23b97dad56d.js"></script>

Below is an example of how we can utilize the vector class.

{% highlight python %}
v1 = vec(2, 2)
v2 = vec(1, 2)
v1 += v2 # (3, 4)
print(v1.magnitude()) # 5.0
{% endhighlight %}


## The Algorithm
Let us take a moment to describe the k-means algorithm in a simple step by step manner.

* Define k - the number of clusters to create
* Create k random points, these will serve as the first “mean” points
* Measure the distance between every data point and the mean points
* Group the points, using their their nearness to the mean points as the predicate
* Find the mean point of each group to create the new means
* If the algorithm has converged (i.e. the new means are the same as the old ones), we are done
* Otherwise, go back to step 3

The most import take away here is that the algorithm will repeat over and over until convergence. On each iteration, the list of means is re-computed. What does it mean to converge? Well, in our example it means that the list of means does not change between iterations.

See how this looks in code!
<script src="https://gist.github.com/jeremynealbrown/b7281486a744ad4a257d.js"></script>

In the example above, we use a hard-coded set of points. These are from the data linked to in the beginning of the article. To represent those points we use the Vec class. It provides all of the functionality we need to find distances from the means as well as methods for adding and dividing to find new means.

In future articles we will use the script above in a more sophisticated manner, by loading data from disk, adding visualizations and possibly even exploring clustering data with higher dimensionality.
